import json
from time import time
from typing import overload
from urllib.parse import urlparse
import uuid
from compliancecowcards.structs import cards
#As per the selected app, we're importing the app package 
from applicationtypes.azureappconnector import azureappconnector
from applicationtypes.sshconnector import sshconnector
from applicationtypes.snykappconnector import snykappconnector
import pandas as pd

class Task(cards.AbstractTask):

    def execute(self) -> dict:
        response = {}
        azure_app = azureappconnector.AzureAppConnector(
            app_url=self.task_inputs.user_object.app.application_url,
            app_port=self.task_inputs.user_object.app.application_port,
            user_defined_credentials=azureappconnector.UserDefinedCredentials.from_dict(
                self.task_inputs.user_object.app.user_defined_credentials)
        )
        
        include_criteria = self.task_inputs.user_inputs.get("IncludeCriteria") or "/registry/*/repo/*/tag/*"
        exclude_criteria = self.task_inputs.user_inputs.get("ExcludeCriteria") or "/registry//repo//tag/"

        registry_details, error = azure_app.get_azure_container_registries_data()
        if error:
            return self.upload_log_file([{ "Error": error }])

        log_messages = []
        filtered_registry_names, error = self.filter_registries_based_on_criteria(registry_details, include_criteria, exclude_criteria)
        if error:
            log_messages.append(error)
        
        repositories_data = {}
        for registry_name in filtered_registry_names:
            repositories, error = azure_app.get_repository_list_using_registry(registry_name)
            if error:
                log_messages.append({"Error": error})
                continue

            repositories_data[registry_name] = repositories

        if not repositories_data:
            log_messages.append({ "Error": "No Repository data found." })
        
        filtered_repositories, error = self.filter_repositories_based_on_criteria(repositories_data, include_criteria, exclude_criteria)
        if error:
            log_messages.append(error)

        if not filtered_repositories:
            log_messages.append({"Error": "No Repository data found after filtering."})
        
        tags_data = {}
        for registry_name, repos in filtered_repositories.items():
            tags_data[registry_name] = {}
            for repo_name in repos:
                tags, error = azure_app.get_tags_data_using_registry_and_repository(registry_name, repo_name)
                if error:
                    log_messages.append({"Error": error})
                    continue
                
                tags_data[registry_name][repo_name] = []  
                if isinstance(tags, dict):
                    for tag in tags.get("tags", []):
                        tags_data[registry_name][repo_name].append({
                            "tag_name": tag["name"]
                        })
                else:
                    log_messages.append({"Error": 'Unexpected response format for tags'})
        
        filtered_tags_data, error = self.filter_tags_based_on_criteria(tags_data, include_criteria, exclude_criteria)
        if error:
            log_messages.append(error)

        if not filtered_tags_data:
            log_messages.append({"Error": "No Tags data found after filtering."})
        else:
            final_df, error = self.generate_final_df(filtered_tags_data)
            if error:
                return self.upload_log_file(error)
            file_name = "TrivyContainerScannerReport"
            scanner_report_file_url, error = self.upload_df_as_parquet_file_to_minio(
                df=final_df,
                file_name=file_name
            )
            if error:
                return { "Error": f"Error while uploading {file_name} file :: {error}" }
            response.update({
                "TrivyContainerScannerReport": scanner_report_file_url
            })

        if log_messages:
            response.update(self.upload_log_file(log_messages))

        return response
    
    def filter_registries_based_on_criteria(self, api_response: list, include_criteria: str, exclude_criteria: str) -> tuple[list, dict] | list:
        include_registries, include_repos, include_tags = self.parse_criteria(include_criteria)
        exclude_registries, exclude_repos, exclude_tags = self.parse_criteria(exclude_criteria)

        if "*" in exclude_registries:
            return None, {"Error" :"Cannot exclude all registries."}

        exclude_set = set()
        for registry in exclude_registries:
            if registry == "*":
                continue
            if "*" in exclude_repos and "*" in exclude_tags:
                exclude_set.add(registry)
            else:
                pass

        available_registries = {registry['name'] for registry in api_response}

        filtered_registries = []
        invalid_include_registries = []
        if "*" in include_registries:
            filtered_registries = list(available_registries - set(exclude_registries))
        else:
            for registry_name in include_registries:
                if registry_name in available_registries and registry_name not in exclude_set:
                    filtered_registries.append(registry_name)
                elif registry_name not in available_registries:
                    invalid_include_registries.append(registry_name)
        
        if invalid_include_registries and filtered_registries:
            return filtered_registries, {"Error": f"Could not find registry/registries with name(s): {', '.join(invalid_include_registries)}"}
        else:
            return filtered_registries, None
    
    def filter_repositories_based_on_criteria(self, repositories_data: dict, include_criteria: str, exclude_criteria: str) -> tuple[dict, dict] | dict:
        include_registries, include_repos, include_tags = self.parse_criteria(include_criteria)
        exclude_registries, exclude_repos, exclude_tags = self.parse_criteria(exclude_criteria)

        if (not exclude_repos and not exclude_tags and exclude_registries) or (exclude_repos and not exclude_tags and exclude_registries) or (not exclude_repos and exclude_tags and exclude_registries):
            return None, {"Error": "Invalid exclude criteria: tag or repo or both is empty."}
        if (not include_repos and not include_tags and include_registries) or (include_repos and not include_tags and include_registries) or (not include_repos and include_tags and include_registries):
            return None, {"Error": "Invalid include criteria: tag or repo or both is empty."}

        filtered_repositories = {}
        excluded_repos_set = set()

        for registry_name, repos in repositories_data.items():
            filtered_repos = repos.copy()

            if "*" in exclude_registries or registry_name in exclude_registries:
                if "*" in exclude_repos:
                    excluded_repos_set.update(repos)
                    continue
                else:
                    for repo in repos:
                        if repo in exclude_repos:
                            excluded_repos_set.update(repos)
                            if "*" in exclude_tags:
                                filtered_repos.remove(repo)
            
            filtered_repositories[registry_name] = filtered_repos

        final_repositories = {}
        invalid_include_repositories = []

        for registry_name, repos in filtered_repositories.items():
            if "*" in include_registries or registry_name in include_registries:
                filtered_repos = []
                for repo in repos:
                    if "*" in include_repos or repo in include_repos:
                        filtered_repos.append(repo)
                    elif repo not in include_repos:
                        continue

                for repo in include_repos:
                    if repo not in repos and repo not in excluded_repos_set and repo != "*":
                        invalid_include_repositories.append(repo)
                if filtered_repos:
                    final_repositories[registry_name] = filtered_repos

        if invalid_include_repositories:
            return final_repositories, {"Error": f"Could not find repository/repositories with name(s): {', '.join(invalid_include_repositories)}"}
        else:   
            return final_repositories, None
    
    def filter_tags_based_on_criteria(self, repositories_with_tags: dict, include_criteria: str, exclude_criteria: str) -> tuple[dict, dict] | dict:
        include_registries, include_repos, include_tags = self.parse_criteria(include_criteria)
        exclude_registries, exclude_repos, exclude_tags = self.parse_criteria(exclude_criteria)

        filtered_repositories_with_tags = {}
        invalid_include_tags = set()

        for registry_name, repositories in repositories_with_tags.items():
            registry_filtered_data = {}

            for repo_name, tags_list in repositories.items():
                remaining_tags = tags_list.copy()

                if (registry_name in exclude_registries or "*" in exclude_registries) and (
                    repo_name in exclude_repos or "*" in exclude_repos):
                    if "*" in exclude_tags:
                        continue  
                    else:
                        remaining_tags = [
                            tag for tag in remaining_tags if tag["tag_name"] not in exclude_tags
                        ]

                if (registry_name in include_registries or "*" in include_registries) and (
                    repo_name in include_repos or "*" in include_repos):
                    
                    if "*" in include_tags:
                        registry_filtered_data[repo_name] = remaining_tags
                    else:
                        valid_tags = [
                            tag for tag in remaining_tags if tag["tag_name"] in include_tags
                        ]

                        for include_tag in include_tags:
                            if not any(tag["tag_name"] == include_tag for tag in tags_list):
                                invalid_include_tags.add(include_tag)
                        if valid_tags:
                            registry_filtered_data[repo_name] = valid_tags

            if registry_filtered_data:
                filtered_repositories_with_tags[registry_name] = registry_filtered_data

        if invalid_include_tags:
            return filtered_repositories_with_tags, {"Error": f"Could not find tag/tags with name(s): {', '.join(invalid_include_tags)}"}
        else:
            return filtered_repositories_with_tags, None

    def parse_criteria(self, criteria: str) -> tuple[list, list, list]:
        registry_criteria, repo_criteria, tag_criteria = [], [], []
        
        if "/registry/" in criteria and "/repo/" in criteria:
            registry_part = criteria.split("/registry/")[1].split("/repo/")[0].strip()
            repo_part = criteria.split("/repo/")[1].split("/tag/")[0].strip()
            tag_part = criteria.split("/tag/")[1].strip()
            
            registry_criteria = registry_part.split(",") if registry_part else []
            repo_criteria = repo_part.split(",") if repo_part else []
            tag_criteria = tag_part.split(",") if tag_part else []
        
        return registry_criteria, repo_criteria, tag_criteria
    
    def get_trivy_cli_credentials(self) -> tuple[dict, str] | dict:
        system_objects = self.task_inputs.system_objects

        if not system_objects:
            return None, 'System Objects are empty. Please contact admin/support to fix this issue.'
        
        for system_object in system_objects:
            if system_object.server and system_object.server.server_name == 'trivy-cli':
                if system_object.credentials:
                    credentials = {
                        'LoginURL': system_object.credentials[0].login_url,
                        'UserName': system_object.credentials[0].user_id,
                        'SSHPrivateKey': system_object.credentials[0].ssh_private_key,
                        'RepositoryType': system_object.credentials[0].cred_tags["type"],
                    }
                    return credentials, None
                else:
                    return None, 'Credentials are missing for trivy-cli server. Please contact admin/support to fix this issue.'

        return None, 'Unable to find the trivy-cli installed server in the system objects. Please contact admin/support to fix this issue.'

    def extract_port_from_url(self, login_url: str) -> str:
        try:
            parsed_url = urlparse(login_url)
            if parsed_url.port:
                return parsed_url.port
            else:
                return "Port not specified in the URL"
        except ValueError as ve:
            return f"ValueError occurred: {str(ve)}"
        except AttributeError as ae:
            return f"AttributeError occurred: {str(ae)}"
        except Exception as e:
            return f"Unexpected error occurred: {str(e)}"
    
    def format_vulnerability_report(self, data: dict) -> pd.DataFrame:
        artifact_name = data.get("ArtifactName", "")
        results = data.get("Results", [])

        flattened_data = [
            {
                "ArtifactName": artifact_name,
                "Type": result.get("Type", "N/A"),
                "VulnerabilityID": vulnerability.get("VulnerabilityID", "N/A"),
                "Severity": vulnerability.get("Severity", "N/A"),
                "PkgName": vulnerability.get("PkgName", "N/A"),
                "PkgID": vulnerability.get("PkgID", "N/A"),
                "InstalledVersion": vulnerability.get("InstalledVersion", "N/A"),
                "FixedVersion": vulnerability.get("FixedVersion", "N/A"),
                "Title": vulnerability.get("Title", "N/A"),
                "Status": vulnerability.get("Status", "N/A"),
                "PublishedDate": vulnerability.get("PublishedDate", "N/A"),
                "LastModifiedDate": vulnerability.get("LastModifiedDate", "N/A"),
            }
            for result in results
            for vulnerability in result.get("Vulnerabilities", [])
        ]

        df = pd.DataFrame(flattened_data)

        snyk_connector = snykappconnector.SnykAppConnector()
        df['EPSSPercentile'] = df['VulnerabilityID'].apply(
            lambda cve_id: snyk_connector.get_epss_score_for_cve(cve_id)[1] if pd.notna(cve_id) else "N/A"
        ).replace("", "N/A")

        df[['ValidationStatusCode', 'ValidationStatusNotes', 'ComplianceStatus', 'ComplianceStatusReason']] = \
            df.apply(lambda row: self.update_compliance_details(row['Severity'], row['EPSSPercentile']), axis=1, result_type="expand")

        final_report_df = df.rename(columns={
            "ArtifactName": "ResourceName",
            "Type": "IssueType",
            "VulnerabilityID": "CVEID",
            "PkgName": "PackageName",
            "PkgID": "PackageID",
            "InstalledVersion": "PackageInstalledVersion",
            "FixedVersion": "PackageFixedVersion",
            "Title": "IssueName",
            "Status": "PlatformStatus",
            "LastModifiedDate": "EvaluatedTime"
        })

        final_report_df = final_report_df.assign(
            System="trivy",
            Source="compliancecow",
            ResourceID="N/A",
            ResourceURL="N/A",
            ResourceType="image",
            ResourceLocation="N/A",
            ResourceTags="N/A",
            UserAction="",
            ActionStatus="",
            ActionResponseURL=""
        )

        column_order = [
            "System", "Source", "ResourceID", "ResourceName", "ResourceType", "ResourceLocation", "ResourceTags",
            "ResourceURL", "IssueName", "IssueType", "PackageName", "PackageID",
            "PackageInstalledVersion", "PackageFixedVersion", "CVEID", "Severity", "EPSSPercentile", "PlatformStatus",
            "ValidationStatusCode", "ValidationStatusNotes", "ComplianceStatus", "ComplianceStatusReason", "EvaluatedTime",
            "UserAction", "ActionStatus", "ActionResponseURL"
        ]
        final_report_df = final_report_df[column_order]

        return final_report_df.to_dict(orient='records')
    
    def generate_final_report(self, all_data: list[dict]) -> list[dict]:
        final_report_df = pd.concat([pd.DataFrame(self.format_vulnerability_report(data)) for data in all_data], ignore_index=True)

        final_report_list = final_report_df.to_dict(orient='records')

        return final_report_list
    
    def update_compliance_details(self, severity: str, epss_percentile: str) -> dict:
        compliance_mapping = {
            ("LOW", "<50"): {"ValidationStatusCode": "LOW_SEV_LOW_EPSS", "ValidationStatusNotes": "Contains a low-severity issue with a low EPSS Percentile (<50).",
                            "ComplianceStatus": "COMPLIANT", "ComplianceStatusReason": "The record is compliant as it has a low-severity vulnerability with a low EPSS percentile."},
            ("LOW", "50-70"): {"ValidationStatusCode": "LOW_SEV_MED_EPSS", "ValidationStatusNotes": "Contains a low-severity issue with a medium EPSS Percentile (50-70).",
                            "ComplianceStatus": "COMPLIANT", "ComplianceStatusReason": "The record is compliant as it has a low-severity vulnerability with a medium EPSS percentile."},
            ("LOW", "70-90"): {"ValidationStatusCode": "LOW_SEV_HIGH_EPSS", "ValidationStatusNotes": "Contains a low-severity issue with a high EPSS Percentile (70-90).",
                            "ComplianceStatus": "NON_COMPLIANT", "ComplianceStatusReason": "The record is non-compliant due to a low-severity vulnerability with a high EPSS percentile."},
            ("LOW", ">90"): {"ValidationStatusCode": "LOW_SEV_CRT_EPSS", "ValidationStatusNotes": "Contains a low-severity issue with a critical EPSS Percentile (>90).",
                            "ComplianceStatus": "NON_COMPLIANT", "ComplianceStatusReason": "The record is non-compliant due to a low-severity vulnerability with a critical EPSS percentile."},
            ("LOW", "NO_EPSS"): {"ValidationStatusCode": "LOW_SEV_NO_EPSS", "ValidationStatusNotes": "Contains a low-severity issue with no EPSS Percentile.",
                            "ComplianceStatus": "COMPLIANT", "ComplianceStatusReason": "The record is compliant due to a low-severity vulnerability with no EPSS percentile."},
            ("MEDIUM", "<50"): {"ValidationStatusCode": "MED_SEV_LOW_EPSS", "ValidationStatusNotes": "Contains a medium-severity issue with a low EPSS Percentile (<50).",
                                "ComplianceStatus": "COMPLIANT", "ComplianceStatusReason": "The record is compliant as it has a medium-severity vulnerability with a low EPSS percentile."},
            ("MEDIUM", "50-70"): {"ValidationStatusCode": "MED_SEV_MED_EPSS", "ValidationStatusNotes": "Contains a medium-severity issue with a medium EPSS Percentile (50-70).",
                                "ComplianceStatus": "COMPLIANT", "ComplianceStatusReason": "The record is compliant as it has a medium-severity vulnerability with a medium EPSS percentile."},
            ("MEDIUM", "70-90"): {"ValidationStatusCode": "MED_SEV_HIGH_EPSS", "ValidationStatusNotes": "Contains a medium-severity issue with a high EPSS Percentile (70-90).",
                                "ComplianceStatus": "NON_COMPLIANT", "ComplianceStatusReason": "The record is non-compliant due to a medium-severity vulnerability with a high EPSS percentile."},
            ("MEDIUM", ">90"): {"ValidationStatusCode": "MED_SEV_CRT_EPSS", "ValidationStatusNotes": "Contains a medium-severity issue with a critical EPSS Percentile (>90).",
                                "ComplianceStatus": "NON_COMPLIANT", "ComplianceStatusReason": "The record is non-compliant due to a medium-severity vulnerability with a critical EPSS percentile."},
            ("MEDIUM", "NO_EPSS"): {"ValidationStatusCode": "MED_SEV_NO_EPSS", "ValidationStatusNotes": "Contains a medium-severity issue with no EPSS Percentile.",
                                "ComplianceStatus": "COMPLIANT", "ComplianceStatusReason": "The record is compliant due to a medium-severity vulnerability with no EPSS percentile."},
            ("HIGH", "<50"): {"ValidationStatusCode": "HIGH_SEV_LOW_EPSS", "ValidationStatusNotes": "Contains a high-severity issue with a low EPSS Percentile (<50).",
                            "ComplianceStatus": "NON_COMPLIANT", "ComplianceStatusReason": "The record is non-compliant due to a high-severity vulnerability with a low EPSS percentile."},
            ("HIGH", "50-70"): {"ValidationStatusCode": "HIGH_SEV_MED_EPSS", "ValidationStatusNotes": "Contains a high-severity issue with a medium EPSS Percentile (50-70).",
                                "ComplianceStatus": "NON_COMPLIANT", "ComplianceStatusReason": "The record is non-compliant due to a high-severity vulnerability with a medium EPSS percentile."},
            ("HIGH", "70-90"): {"ValidationStatusCode": "HIGH_SEV_HIGH_EPSS", "ValidationStatusNotes": "Contains a high-severity issue with a high EPSS Percentile (70-90).",
                                "ComplianceStatus": "NON_COMPLIANT", "ComplianceStatusReason": "The record is non-compliant due to a high-severity vulnerability with a high EPSS percentile."},
            ("HIGH", ">90"): {"ValidationStatusCode": "HIGH_SEV_CRT_EPSS", "ValidationStatusNotes": "Contains a high-severity issue with a critical EPSS Percentile (>90).",
                            "ComplianceStatus": "NON_COMPLIANT", "ComplianceStatusReason": "The record is non-compliant due to a high-severity vulnerability with a critical EPSS percentile."},
            ("HIGH", "NO_EPSS"): {"ValidationStatusCode": "HIGH_SEV_NO_EPSS", "ValidationStatusNotes": "Contains a high-severity issue with no EPSS Percentile.",
                              "ComplianceStatus": "NON_COMPLIANT", "ComplianceStatusReason": "The record is non-compliant due to a high-severity vulnerability with no EPSS percentile."},
            ("CRITICAL", "<50"): {"ValidationStatusCode": "CRT_SEV_LOW_EPSS", "ValidationStatusNotes": "Contains a critical-severity issue with a low EPSS Percentile (<50).",
                                "ComplianceStatus": "NON_COMPLIANT", "ComplianceStatusReason": "The record is non-compliant due to a critical-severity vulnerability with a low EPSS percentile."},
            ("CRITICAL", "50-70"): {"ValidationStatusCode": "CRT_SEV_MED_EPSS", "ValidationStatusNotes": "Contains a critical-severity issue with a medium EPSS Percentile (50-70).",
                                    "ComplianceStatus": "NON_COMPLIANT", "ComplianceStatusReason": "The record is non-compliant due to a critical-severity vulnerability with a medium EPSS percentile."},
            ("CRITICAL", "70-90"): {"ValidationStatusCode": "CRT_SEV_HIGH_EPSS", "ValidationStatusNotes": "Contains a critical-severity issue with a high EPSS Percentile (70-90).",
                                    "ComplianceStatus": "NON_COMPLIANT", "ComplianceStatusReason": "The record is non-compliant due to a critical-severity vulnerability with a high EPSS percentile."},
            ("CRITICAL", ">90"): {"ValidationStatusCode": "CRT_SEV_CRT_EPSS", "ValidationStatusNotes": "Contains a critical-severity issue with a critical EPSS Percentile (>90).",
                                "ComplianceStatus": "NON_COMPLIANT", "ComplianceStatusReason": "The record is non-compliant due to a critical-severity vulnerability with a critical EPSS percentile."},
            ("CRITICAL", "NO_EPSS"): {"ValidationStatusCode": "CRT_SEV_NO_EPSS", "ValidationStatusNotes": "Contains a critical-severity issue with no EPSS Percentile.",
                                  "ComplianceStatus": "NON_COMPLIANT", "ComplianceStatusReason": "The record is non-compliant due to a critical-severity vulnerability with no EPSS percentile."},
        }

        epss_band = "NO_EPSS"
        if epss_percentile is not None:
            try:
                epss_percentile = float(epss_percentile)
                if epss_percentile < 50:
                    epss_band = "<50"
                elif epss_percentile < 70:
                    epss_band = "50-70"
                elif epss_percentile < 90:
                    epss_band = "70-90"
                else:
                    epss_band = ">90"
            except ValueError:
                epss_band = "NO_EPSS"

        return compliance_mapping.get((severity.upper(), epss_band), {})
    
    def generate_final_df(self, filtered_tags_data: dict) -> tuple[pd.DataFrame, list[dict]]:
        trivy_credentials, error = self.get_trivy_cli_credentials()
        if error:
           return self.upload_log_file([{"Error": error}])

        ssh_data = {
            'UserName': trivy_credentials['UserName'],
            'SSHKey': trivy_credentials['SSHPrivateKey']
        }
        ssh_instance = sshconnector.SSH.from_dict(ssh_data)
        self.ssh_connector = sshconnector.SSHConnector(
            app_url=trivy_credentials['LoginURL'],
            app_port = self.extract_port_from_url(trivy_credentials['LoginURL']),
            user_defined_credentials=sshconnector.UserDefinedCredentials(
                ssh_instance)
        )
        client_id = self.task_inputs.user_object.app.user_defined_credentials["Azure"].get("clientID")
        client_secret = self.task_inputs.user_object.app.user_defined_credentials["Azure"].get("clientSecret")

        error_logs = []
        all_data = []
        for registry_name, repositories in filtered_tags_data.items():
            for repo_name, tags in repositories.items():
                for tag_info in tags:
                    tag_name = tag_info['tag_name']
                    meta_data = self.task_inputs.meta_data
                    report_file_path = f'TrivyFile-{str(uuid.uuid4())}.json'
                    if trivy_credentials['RepositoryType'] == ["AZURE"]:
                        cmd = (
                            f"export TRIVY_AUTH_URL={registry_name}.azurecr.io && "
                            f"export TRIVY_USERNAME={client_id} &&"
                            f"export TRIVY_PASSWORD={client_secret} &&"
                            f"trivy image -f json -o {report_file_path} {registry_name}.azurecr.io/{repo_name}:{tag_name}"
                        )
                    else:
                        cmd = (
                            f"trivy image -f json -o {report_file_path} {repo_name}:{tag_name}"
                        )
                    
                    command_output, error, exit_status = self.ssh_connector.exec_command(cmd)
                    if error:
                        error_logs.append({"Error executing command": cmd, "Error message": error})
                        continue
                    if 'FATAL' in command_output or 'UNAUTHORIZED' in command_output:
                        error_logs.append({"Error occurred in command output": command_output})
                        continue

                    try:
                        file_content = self.ssh_connector.fetch_file(report_file_path)
                        if file_content is not None:
                            data = json.loads(file_content)
                            all_data.append(data)
                        else:
                            error_logs.append({"Error": f"File {report_file_path} is empty or could not be fetched."})
                    except RuntimeError as e:
                        error_logs.append({"Error": f"Error fetching file {report_file_path}, Error message: {str(e)}"})
                    finally:
                        remove_status, remove_error = self.ssh_connector.remove_remote_file(report_file_path)
                        if remove_error:
                            error_logs.append({"Error removing remote file": report_file_path, "Error message": remove_error})
                    
            final_report = self.generate_final_report(all_data)

        final_df = pd.DataFrame(final_report)
        return final_df, error_logs
    
    def upload_log_file(self, error_data: list | dict) -> dict:
        if not isinstance(error_data, list):
            error_data = [error_data]
        log_file_content = json.dumps(error_data, indent=4)
        file_name = f"LogFile-{str(uuid.uuid4())}.json"
        content_type = "application/json"
        file_url, error = self.upload_file_to_minio(
            file_content=log_file_content.encode('utf-8'),
            file_name=file_name,
            content_type=content_type
        )
        if error:
            return { "Error": f"Error while uploading LogFile: {error}"}
        return { "LogFile": file_url }